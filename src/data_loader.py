import requests
import pandas as pd
import time
from datetime import datetime, timedelta

# ==========================================
# é…ç½®åŒº
# ==========================================
# æˆ‘ä»¬è¦å…³æ³¨çš„å…¬å¸/å…³é”®è¯
KEYWORDS = [
    'OpenAI',        # åŸºå‡† (Benchmark)
    'Google Gemini', # å¤§å‚ç«å“ (Big Tech)
    'DeepSeek',      # ğŸ”¥ å½“å‰æœ€çƒ­ (Trend / High Moat?)
    'Perplexity',    # åº”ç”¨å±‚ä»£è¡¨ (App Layer)
    'LangChain',     # ä¸­é—´ä»¶ (Middleware / "Wrapper" Debate)
    'AI Agents'      # çƒ­é—¨è¯é¢˜ (Future Narrative)
]
# çˆ¬å–è¿‡å»å¤šå°‘å¤©çš„æ•°æ®
DAYS_BACK = 500
# æ¯æ¬¡è¯·æ±‚çš„é—´éš”ï¼ˆç§’ï¼‰ï¼Œé˜²æ­¢è¢«å° IP
SLEEP_TIME = 1

def fetch_hn_data(keyword, days_back):
    """
    ä½¿ç”¨ Algolia API æœç´¢ Hacker News ä¸Šçš„ç›¸å…³è¯„è®º
    """
    print(f"ğŸ” Searching for: {keyword}...")
    
    # è®¡ç®—æ—¶é—´æˆ³
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days_back)
    numeric_filter = f"created_at_i>{int(start_date.timestamp())}"
    
    # Hacker News Algolia API URL
    url = "http://hn.algolia.com/api/v1/search_by_date"
    
    params = {
        'query': keyword,
        'tags': 'comment',       # åªæŠ“è¯„è®ºï¼Œä¸æŠ“æ–°é—»æ ‡é¢˜
        'numericFilters': numeric_filter,
        'hitsPerPage': 1000       # æ”¹å¤§ï¼è®©æ•°æ®æ›´å¯†é›†ï¼Œæ—¶é—´è·¨åº¦æ›´é•¿
    }
    
    try:
        response = requests.get(url, params=params)
        data = response.json()
        
        # æå–æœ‰ç”¨å­—æ®µ
        comments = []
        for hit in data['hits']:
            comments.append({
                'date': hit['created_at'],
                'company': keyword,
                'comment_text': hit['comment_text'], # è¿˜æ²¡æ¸…æ´—çš„åŸå§‹æ–‡æœ¬
                'author': hit['author'],
                'points': hit.get('points', 0), # çˆ¬å–ç‚¹èµæ•°ï¼Œå¦‚æœæ²¡æœ‰å°±å¡«0
                'objectID': hit['objectID']
                
            })
        
        print(f"   âœ… Found {len(comments)} comments.")
        return comments
    
    except Exception as e:
        print(f"   âŒ Error fetching {keyword}: {e}")
        return []

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
if __name__ == "__main__":
    all_data = []
    
    print("ğŸš€ Starting Data Pipeline...")
    
    for company in KEYWORDS:
        company_data = fetch_hn_data(company, DAYS_BACK)
        all_data.extend(company_data)
        time.sleep(SLEEP_TIME) # ç¤¼è²Œçˆ¬è™«
    
    # è½¬ä¸º DataFrame
    df = pd.DataFrame(all_data)
    
    # ç®€å•æ¸…æ´—ï¼šå»é™¤ç©ºæ–‡æœ¬
    df = df[df['comment_text'].notna()]
    
    # è½¬æ¢æ—¶é—´æ ¼å¼
    df['date'] = pd.to_datetime(df['date'])
    
    # é¢„è§ˆ
    print("\nğŸ“Š Data Summary:")
    print(df.groupby('company').size())
    
    # ä¿å­˜ä¸ºâ€œåŸå§‹æ•°æ®â€ (Raw Data)
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å­˜ä¸º raw_hn_data.csvï¼Œä¸è¦†ç›–ä¹‹å‰çš„ mock_data.csv
    # å› ä¸ºè¿™ä¸ªæ–‡ä»¶é‡Œè¿˜æ²¡æœ‰ scoreï¼Œç›´æ¥å–‚ç»™ Dashboard ä¼šæŠ¥é”™
    filename = 'data/raw_hn_data.csv'
    df.to_csv(filename, index=False)
    print(f"\nğŸ’¾ Raw data saved to {filename}. Next step: NLP Scoring.")